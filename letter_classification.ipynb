{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "import shutil\n",
    "import matplotlib.image as mpimg\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model / data parameters\n",
    "img_width, img_height = 28, 28\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "input_shape = (28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notMNIST_load_data is a function that loads the notMNIST dataset. It returns three tuples containing the training, validation and test data. Each tuple is formed by a numpy array containing the images and a numpy array containing the labels (0 to 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def notMNIST_load_data() : \n",
    "    data_dir_letters = 'data/notMNIST_small'\n",
    "\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "\n",
    "    # we get the number of samples in the smallest class\n",
    "    min_nb_samples = float('inf')\n",
    "    for letter in os.listdir(data_dir_letters):\n",
    "        nb_samples = len(os.listdir(os.path.join(data_dir_letters, letter)))\n",
    "        min_nb_samples = min(min_nb_samples, nb_samples)\n",
    "\n",
    "    # 80% of the data is used for training\n",
    "    # 10% for validation\n",
    "    # 10% for testing\n",
    "    nb_train_samples = int(min_nb_samples * 0.8)\n",
    "    # nb_validation_samples = int(min_nb_samples * 0.1)\n",
    "    nb_validation_samples = 0\n",
    "    nb_test_samples = nb_samples - nb_train_samples - nb_validation_samples\n",
    "\n",
    "    # for each letter folder, we copy the images in the train, validation or test tuple and the label\n",
    "    # TODO : randomize the order of the images would be better ?\n",
    "    for letter in os.listdir(data_dir_letters):\n",
    "        index = 0\n",
    "\n",
    "        for image in glob.iglob(os.path.join(data_dir_letters, letter, \"*.png\")):\n",
    "\n",
    "            if index < nb_train_samples:\n",
    "                pixels_array = mpimg.imread(image)\n",
    "                x_train.append(pixels_array)\n",
    "                y_train.append(ord(letter) - 65)\n",
    "            #elif index < nb_train_samples + nb_validation_samples:\n",
    "                \n",
    "            elif index < nb_train_samples + nb_validation_samples + nb_test_samples:\n",
    "                pixels_array = mpimg.imread(image)\n",
    "                x_test.append(pixels_array)\n",
    "                y_test.append(ord(letter) - 65)\n",
    "            index += 1\n",
    "    \n",
    "    return (np.array(x_train), np.array(y_train)), (np.array(x_test), np.array(y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
