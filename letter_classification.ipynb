{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "import matplotlib.image as mpimg\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model / data parameters\n",
    "img_width, img_height = 28, 28\n",
    "epochs = 15\n",
    "batch_size = 16\n",
    "input_shape = (28, 28, 1)\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notMNIST_load_data is a function that loads the notMNIST dataset. It returns three tuples containing the training, validation and test data. Each tuple is formed by a numpy array containing the images and a numpy array containing the labels (0 to 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def notMNIST_load_data() : \n",
    "    data_dir_letters = 'data/notMNIST_small'\n",
    "\n",
    "    nb_letters = len(os.listdir(data_dir_letters))\n",
    "    if nb_letters != num_classes:\n",
    "        raise ValueError('The number of classes is not equal to the number of letters in the folder')\n",
    "\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "\n",
    "    # we get the number of samples in the smallest class\n",
    "    min_nb_samples = float('inf')\n",
    "    for letter in os.listdir(data_dir_letters):\n",
    "        nb_samples = len(os.listdir(os.path.join(data_dir_letters, letter)))\n",
    "        min_nb_samples = min(min_nb_samples, nb_samples)\n",
    "\n",
    "    # 80% of the data is used for training\n",
    "    # 10% for validation\n",
    "    # 10% for testing\n",
    "    nb_train_samples = int(min_nb_samples * 0.8)\n",
    "    # nb_validation_samples = int(min_nb_samples * 0.1)\n",
    "    nb_validation_samples = 0\n",
    "    nb_test_samples = nb_samples - nb_train_samples - nb_validation_samples\n",
    "\n",
    "    # for each letter folder, we copy the images in the train, validation or test tuple and the label\n",
    "    # TODO : randomize the order of the images would be better ?\n",
    "    for letter in os.listdir(data_dir_letters):\n",
    "        index = 0\n",
    "\n",
    "        for image in glob.iglob(os.path.join(data_dir_letters, letter, \"*.png\")):\n",
    "\n",
    "            if index < nb_train_samples:\n",
    "                pixels_array = mpimg.imread(image)\n",
    "                x_train.append(pixels_array)\n",
    "                y_train.append(ord(letter) - 65)\n",
    "            #elif index < nb_train_samples + nb_validation_samples:\n",
    "                \n",
    "            elif index < nb_train_samples + nb_validation_samples + nb_test_samples:\n",
    "                pixels_array = mpimg.imread(image)\n",
    "                x_test.append(pixels_array)\n",
    "                y_test.append(ord(letter) - 65)\n",
    "            index += 1\n",
    "    \n",
    "    return (np.array(x_train), np.array(y_train)), (np.array(x_test), np.array(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = notMNIST_load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (14970, 28, 28, 1)\n",
      "14970 train samples\n",
      "3750 test samples\n"
     ]
    }
   ],
   "source": [
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=64, activation='relu'))\n",
    "    model.add(Dropout(rate=0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_LeNet():\n",
    "    # LeNet-5 architecture\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(filters=6, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(AveragePooling2D())\n",
    "\n",
    "    model.add(Conv2D(filters=16, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(AveragePooling2D())\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(units=120, activation='relu'))\n",
    "    model.add(Dense(units=84, activation='relu'))\n",
    "    model.add(Dense(units=num_classes, activation = 'softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_LeNet_new():\n",
    "    # LeNet-5 architecture\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(filters=6, kernel_size=(5, 5), activation='tanh', input_shape=input_shape))\n",
    "    model.add(AveragePooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(filters=16, kernel_size=(5, 5), activation='tanh'))\n",
    "    model.add(AveragePooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(units=120, activation='tanh'))\n",
    "    model.add(Dense(units=84, activation='tanh'))\n",
    "    model.add(Dense(units=num_classes, activation = 'softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'build_model_LeNet_new' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# get the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model_LeNet_new\u001b[49m()\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'build_model_LeNet_new' is not defined"
     ]
    }
   ],
   "source": [
    "# get the model\n",
    "model = build_model_LeNet_new()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\", \n",
    "    optimizer=\"rmsprop\", \n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "843/843 [==============================] - 5s 4ms/step - loss: 1.0596 - accuracy: 0.6654 - val_loss: 9.1346 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/15\n",
      "843/843 [==============================] - 3s 3ms/step - loss: 0.5684 - accuracy: 0.8510 - val_loss: 11.7978 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/15\n",
      "843/843 [==============================] - 3s 3ms/step - loss: 0.4694 - accuracy: 0.8677 - val_loss: 11.4186 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/15\n",
      "843/843 [==============================] - 2s 3ms/step - loss: 0.4153 - accuracy: 0.8780 - val_loss: 10.4892 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/15\n",
      "843/843 [==============================] - 3s 3ms/step - loss: 0.3814 - accuracy: 0.8840 - val_loss: 10.1729 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/15\n",
      "843/843 [==============================] - 3s 3ms/step - loss: 0.3526 - accuracy: 0.8959 - val_loss: 10.3835 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/15\n",
      "843/843 [==============================] - 3s 3ms/step - loss: 0.3280 - accuracy: 0.9003 - val_loss: 9.7597 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/15\n",
      "843/843 [==============================] - 3s 3ms/step - loss: 0.3077 - accuracy: 0.9035 - val_loss: 9.2530 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/15\n",
      "843/843 [==============================] - 2s 3ms/step - loss: 0.2867 - accuracy: 0.9120 - val_loss: 9.1892 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/15\n",
      "843/843 [==============================] - 2s 3ms/step - loss: 0.2650 - accuracy: 0.9180 - val_loss: 9.0587 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/15\n",
      "843/843 [==============================] - 3s 3ms/step - loss: 0.2498 - accuracy: 0.9233 - val_loss: 9.0457 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/15\n",
      "843/843 [==============================] - 2s 3ms/step - loss: 0.2345 - accuracy: 0.9266 - val_loss: 8.9233 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/15\n",
      "843/843 [==============================] - 2s 3ms/step - loss: 0.2183 - accuracy: 0.9312 - val_loss: 9.1499 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/15\n",
      "843/843 [==============================] - 2s 3ms/step - loss: 0.2028 - accuracy: 0.9353 - val_loss: 8.7066 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/15\n",
      "843/843 [==============================] - 2s 3ms/step - loss: 0.1896 - accuracy: 0.9409 - val_loss: 9.3320 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1c786963410>"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.2189500331878662\n",
      "Test accuracy: 0.81413334608078\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results - used to find the best architecture and hyperparameters\n",
    "* Architecture LeNet-5 \\\n",
    "50 epochs, batch size 128 \\\n",
    "Epoch 50/50 - loss: 0.3308 - accuracy: 0.8945 \\\n",
    "Test loss: 2.313185691833496 \\\n",
    "Test accuracy: 0.649066686630249 \\\n",
    "\n",
    "* Architecture LeNet-5 \\\n",
    "15 epochs, batch size 16 \\\n",
    "Epoch 15/15 - loss: 0.4077 - accuracy: 0.8687 \\\n",
    "Test loss: 2.001655101776123 \\\n",
    "Test accuracy: 0.7760000228881836 \\\n",
    "\n",
    "* Architecture LeNet-5 \\\n",
    "15 epochs, batch size 16 \\\n",
    "Convolution : kernel_size=(5, 5), activation='relu' \\\n",
    "Epoch 15/15 - loss: 0.2376 - accuracy: 0.9270 \\\n",
    "Test loss: 1.7583626508712769 \\\n",
    "Test accuracy: 0.8191999793052673 \\\n",
    "\n",
    "* Architecture LeNet-5_new \\\n",
    "15 epochs, batch size 16 \\\n",
    "Convolution : kernel_size=(5, 5), activation='tanh' \\\n",
    "Pooling : pool_size=(2, 2), strides=(2, 2) \\\n",
    "Epoch 15/15 - loss: 0.2376 - accuracy: 0.9270 \\\n",
    "Test loss: 1.7583626508712769   \\\n",
    "Test accuracy: 0.8191999793052673 \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
