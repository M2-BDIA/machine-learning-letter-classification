{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from tensorflow import keras\n",
    "import shutil\n",
    "import matplotlib.image as mpimg\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model / data parameters\n",
    "img_width, img_height = 28, 28\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "input_shape = (28, 28, 1)\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notMNIST_load_data is a function that loads the notMNIST dataset. It returns three tuples containing the training, validation and test data. Each tuple is formed by a numpy array containing the images and a numpy array containing the labels (0 to 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def notMNIST_load_data() : \n",
    "    data_dir_letters = 'data/notMNIST_small'\n",
    "\n",
    "    nb_letters = len(os.listdir(data_dir_letters))\n",
    "    if nb_letters != num_classes:\n",
    "        raise ValueError('The number of classes is not equal to the number of letters in the folder')\n",
    "\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "\n",
    "    # we get the number of samples in the smallest class\n",
    "    min_nb_samples = float('inf')\n",
    "    for letter in os.listdir(data_dir_letters):\n",
    "        nb_samples = len(os.listdir(os.path.join(data_dir_letters, letter)))\n",
    "        min_nb_samples = min(min_nb_samples, nb_samples)\n",
    "\n",
    "    # 80% of the data is used for training\n",
    "    # 10% for validation\n",
    "    # 10% for testing\n",
    "    nb_train_samples = int(min_nb_samples * 0.8)\n",
    "    # nb_validation_samples = int(min_nb_samples * 0.1)\n",
    "    nb_validation_samples = 0\n",
    "    nb_test_samples = nb_samples - nb_train_samples - nb_validation_samples\n",
    "\n",
    "    # for each letter folder, we copy the images in the train, validation or test tuple and the label\n",
    "    # TODO : randomize the order of the images would be better ?\n",
    "    for letter in os.listdir(data_dir_letters):\n",
    "        index = 0\n",
    "\n",
    "        for image in glob.iglob(os.path.join(data_dir_letters, letter, \"*.png\")):\n",
    "\n",
    "            if index < nb_train_samples:\n",
    "                pixels_array = mpimg.imread(image)\n",
    "                x_train.append(pixels_array)\n",
    "                y_train.append(ord(letter) - 65)\n",
    "            #elif index < nb_train_samples + nb_validation_samples:\n",
    "                \n",
    "            elif index < nb_train_samples + nb_validation_samples + nb_test_samples:\n",
    "                pixels_array = mpimg.imread(image)\n",
    "                x_test.append(pixels_array)\n",
    "                y_test.append(ord(letter) - 65)\n",
    "            index += 1\n",
    "    \n",
    "    return (np.array(x_train), np.array(y_train)), (np.array(x_test), np.array(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = notMNIST_load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (14970, 28, 28, 1)\n",
      "14970 train samples\n",
      "3750 test samples\n"
     ]
    }
   ],
   "source": [
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
